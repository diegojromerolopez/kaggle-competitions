{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import tree\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_train_df = pd.read_csv(\"data/train.csv\")\n",
    "source_test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derived value extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set some data as categorical if it indeed is\n",
    "source_train_df[\"Pclass\"] = source_train_df[\"Pclass\"].astype('category')\n",
    "source_test_df[\"Pclass\"] = source_test_df[\"Pclass\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_name_derived_features(df):\n",
    "    df[\"FamilyName\"] = df[\"Name\"].map(lambda name: name.split(\",\")[0])\n",
    "    df[\"NameHasQuotes\"] = df[\"Name\"].map(lambda name: 1 if name.find(\"\\\"\") != -1 else 0)\n",
    "    df[\"NameHasParentheses\"] = df[\"Name\"].map(lambda name: 1 if name.find(\"(\") != -1 else 0)\n",
    "\n",
    "def add_title(df):\n",
    "    \n",
    "    def _get_title(name):\n",
    "        title_and_name = name.split(\",\")[1]\n",
    "        return title_and_name.split(\".\")[0].strip()\n",
    "    \n",
    "    df[\"Title\"] = df[\"Name\"].map(lambda name : _get_title(name) )\n",
    "    \n",
    "    title_to_title_family = {\n",
    "         \"Capt\":       \"officer\",\n",
    "         \"Col\":        \"officer\",\n",
    "         \"Major\":      \"officer\",\n",
    "         \"Dr\":         \"officer\",\n",
    "         \"Rev\":        \"officer\",\n",
    "         \"Jonkheer\":   \"snob\",\n",
    "         \"Don\":        \"snob\",\n",
    "         \"Sir\" :       \"snob\",\n",
    "         \"the Countess\":\"snob\",\n",
    "         \"Dona\":       \"snob\",\n",
    "         \"Lady\" :      \"snob\",\n",
    "         \"Mme\":        \"married\",\n",
    "         \"Ms\":         \"married\",\n",
    "         \"Mrs\" :       \"married\",\n",
    "         \"Miss\" :      \"single\",\n",
    "         \"Mlle\":       \"single\",\n",
    "         \"Mr\" :        \"man\",\n",
    "         \"Master\" :    \"boy\"\n",
    "    }\n",
    "    \n",
    "    df[\"TitleGroup\"] = df[\"Title\"].map(lambda title: title_to_title_family[title])\n",
    "    # Fill age nan for each group\n",
    "    median_age_by_pclass_and_title = defaultdict(dict)\n",
    "    for pclass in [1, 2, 3]:\n",
    "        for title in title_to_title_family.keys():\n",
    "            median_age_by_pclass_and_title[pclass][title] = df[(df.Pclass == pclass) & (df.Title == title)][\"Age\"].mean()\n",
    "    \n",
    "    def set_age(tuple):\n",
    "        if tuple[\"Age\"] is np.nan:\n",
    "            tuple[\"Age\"] = median_age_by_pclass_and_title[tuple[\"Pclass\"]][tuple[\"Title\"]]\n",
    "        return tuple\n",
    "    \n",
    "    df[\"Age\"] = [median_age_by_title[row[\"Title\"]] if row.Age is np.nan else row[\"Age\"] \n",
    "                        for _, row in df.iterrows()]\n",
    "    \n",
    "    df[\"isChild\"] = [1. if row.Age < 15 else 0. for _, row in df.iterrows()]\n",
    "    df[\"isOld\"] = [1. if row.Age >= 65 else 0. for _, row in df.iterrows()]\n",
    "\n",
    "\n",
    "# Add cabin deck\n",
    "def add_deck(df):\n",
    "    \n",
    "    df['Cabin'] = df['Cabin'].map(lambda x : \"Unknown\" if x == \"\" or x is np.nan else x)\n",
    "    df['Deck_Unknown'] = df['Cabin'].map(lambda x : 1 if x == \"Unknown\" else 0)\n",
    "    \n",
    "    deck_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G']\n",
    "    for deck in deck_list:\n",
    "        df['Deck_{}'.format(deck)] = df['Cabin'].map(lambda x : 1 if deck in x else 0)\n",
    "\n",
    "\n",
    "def add_room_number(df):\n",
    "    \n",
    "    def _room_number(cabin):\n",
    "        result = re.compile(r\"([0-9]+)\").search(cabin)\n",
    "        if result:\n",
    "            return result.group()\n",
    "        return 0\n",
    "    \n",
    "    df['Room'] = df['Cabin'].map(lambda cell: _room_number(cell) if cell != \"Unknown\" else np.nan).astype(float) \n",
    "    \n",
    "    def _has_room(room):\n",
    "        if room is np.nan or math.isnan(room):\n",
    "            return 0\n",
    "        return 1\n",
    "    \n",
    "    df['HasRoom'] = df['Room'].map(_has_room)\n",
    "    room_mean = df['Room'].mean()\n",
    "    df['Room'] = df['Room'].fillna(room_mean)\n",
    "    df['Room'] = df['Room'].astype(int)\n",
    "    df['RoomInFront'] = df['Room'].map(lambda cell: 1 if cell <= room_mean else 0)\n",
    "    df['RoomInBack'] = df['Room'].map(lambda cell: 1 if cell > room_mean else 0)\n",
    "            \n",
    "\n",
    "def add_family_size(df):\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch']\n",
    "    df['Singleton'] = df['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    df['SmallFamily'] = df['FamilySize'].map(lambda s: 1 if 2<=s<=4 else 0)\n",
    "    df['LargeFamily'] = df['FamilySize'].map(lambda s: 1 if 5<=s else 0)\n",
    "\n",
    "\n",
    "def add_fare_per_person(df):\n",
    "    df['FarePerPerson'] = df['Fare']/(df['FamilySize']+1)\n",
    "\n",
    "    \n",
    "def add_ticket_derived_features(df):\n",
    "    ticket_count = df[\"Ticket\"].value_counts()\n",
    "    df[\"TicketCount\"] = df[\"Ticket\"].map(lambda ticket : ticket_count[ticket])\n",
    "    df[\"TicketPrefix\"] = df[\"Ticket\"].map(lambda ticket : ticket.split(\" \")[0] if \" \" in ticket else \"\")\n",
    "    df[\"TicketNumber\"] = df[\"Ticket\"].map(lambda ticket : ticket.split(\" \")[1] if \" \" in ticket else ticket)\n",
    "    \n",
    "    def _get_first_digit(ticket_number):\n",
    "        if re.match(r\"\\d+\", ticket_number):\n",
    "            return \"{}\".format(ticket_number)[0]\n",
    "        return np.nan\n",
    "    \n",
    "    df[\"TicketFirstDigit\"] = df[\"TicketNumber\"].map(_get_first_digit).astype(float)\n",
    "\n",
    "\n",
    "def add_new_features(df):\n",
    "    add_name_derived_features(df)\n",
    "    add_title(df)\n",
    "    add_deck(df)\n",
    "    add_family_size(df)\n",
    "    add_fare_per_person(df)\n",
    "    add_room_number(df)\n",
    "    add_ticket_derived_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_new_features(source_train_df)\n",
    "add_new_features(source_test_df)\n",
    "\n",
    "ticket_prefix_categories = list(source_train_df[\"TicketPrefix\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "source_train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fare\n",
    "# It is known that fare == 0 is an error, so will be replacing it to nan\n",
    "source_train_df[\"Fare\"] = source_train_df[\"Fare\"].map(lambda _fare: np.nan if _fare == 0.0 else _fare)\n",
    "source_test_df[\"Fare\"] = source_test_df[\"Fare\"].map(lambda _fare: np.nan if _fare == 0.0 else _fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace nan values\n",
    "def replace_nan_values(df):\n",
    "    return df.fillna(df.median())\n",
    "    \n",
    "train_no_nans_df = replace_nan_values(source_train_df)\n",
    "test_no_nans_df = replace_nan_values(source_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop columns we are not interested\n",
    "columns_to_drop = [\"Name\", \"Ticket\", \"Cabin\"]\n",
    "\n",
    "simplified_train_df = train_no_nans_df.drop(columns_to_drop, axis=1)\n",
    "simplified_test_df = test_no_nans_df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nominal attributes are changed to values\n",
    "categorical_columns = [\"Pclass\", \"Sex\", \"Embarked\", \"Title\", \"TitleGroup\", \"TicketPrefix\"]\n",
    "\n",
    "expanded_train_df = pd.get_dummies(simplified_train_df, columns=categorical_columns)\n",
    "expanded_columns = expanded_train_df.columns.values.tolist()\n",
    "\n",
    "expanded_test_df = pd.get_dummies(simplified_test_df, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case any column of test is not present in train, set it to zero\n",
    "all_columns = set(expanded_train_df.columns).union(set(expanded_test_df.columns))\n",
    "for column in all_columns:\n",
    "    if column not in expanded_train_df.columns:\n",
    "        expanded_train_df[column] = 0\n",
    "    if column not in expanded_test_df.columns:\n",
    "        expanded_test_df[column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_train_df.columns)\n",
    "print(expanded_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(expanded_train_df.columns) == len(expanded_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare raw data for algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features that will have a role in the classification\n",
    "selected_features = [\n",
    "    \"PassengerId\",\n",
    "    \"Age\",\n",
    "    \"Pclass_1\", \"Pclass_2\", \"Pclass_3\",\n",
    "    \"Sex_female\", \"Sex_male\",\n",
    "    #\"NameHasQuotes\", \"NameHasParentheses\",\n",
    "    \"Fare\",\n",
    "    \"FarePerPerson\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"TicketCount\",\n",
    "    #\"TicketFirstDigit\",\n",
    "    \"FamilySize\",\n",
    "    \"Singleton\",\n",
    "    \"SmallFamily\",\n",
    "    \"LargeFamily\",\n",
    "    \"HasRoom\",\n",
    "    \"Room\",\n",
    "    \"RoomInBack\",\n",
    "    \"RoomInFront\",\n",
    "    \"isChild\",\n",
    "    \"isOld\",\n",
    "    \"Deck_A\", \"Deck_B\", \"Deck_C\", \"Deck_D\", \"Deck_E\", \"Deck_F\", \"Deck_T\", \"Deck_G\"\n",
    "] +\\\n",
    "[\"TitleGroup_{}\".format(family_group) for family_group in [\"officer\", \"snob\", \"married\", \"single\", \"man\", \"boy\"]] +\\\n",
    "[\"TicketPrefix_{}\".format(ticket_prefix) for ticket_prefix in ticket_prefix_categories]\n",
    "\n",
    "# X and Y are the input and output of the classifier algorithm\n",
    "train_y = train_no_nans_df.Survived.astype(int).values\n",
    "\n",
    "# test_x and train_x must have the same number of columns and\n",
    "# test_x has no \"Survived\" column so we must drop it from train_x\n",
    "train_x = expanded_train_df[selected_features].values\n",
    "\n",
    "test_x = expanded_test_df[selected_features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic decision tree Pipeline\n",
    "classifier = Pipeline([\n",
    "        ('minmaxscaler', MinMaxScaler()),\n",
    "        ('classifier', NearestCentroid())\n",
    "    ])\n",
    "\n",
    "parameters = {\n",
    "    'classifier__metric': [\"manhattan\", \"euclidean\"], \n",
    "    'classifier__shrink_threshold': [None, .05, .1, .2, .5, .55, .6, .7, .74, .75, .77, .8, .85, .9, 1],\n",
    "}\n",
    "\n",
    "nearest_centroid_clf = GridSearchCV(classifier, parameters, cv=5)\n",
    "\n",
    "nearest_centroid_clf.fit(train_x, train_y)\n",
    "\n",
    "print (\"Best parameters found: \")\n",
    "print (nearest_centroid_clf.best_params_)\n",
    "\n",
    "NFOLDS = 5\n",
    "scores = cross_val_score(nearest_centroid_clf.best_estimator_, train_x, train_y, cv=NFOLDS)\n",
    "print (\"Expected performance: {:.2f}% (+/-{:.2f}).\".format(np.mean(scores)*100., np.std(scores)*100.))\n",
    "\n",
    "nearest_centroid_prediction = nearest_centroid_clf.predict(test_x)\n",
    "\n",
    "# Add the prediction to the test dataset\n",
    "test_classified_with_nearest_centroid = source_test_df.assign(Survived=list(nearest_centroid_prediction))\n",
    "\n",
    "# Save to upload to Kaggle\n",
    "test_classified_with_nearest_centroid.to_csv(\"results/test_classified_with_nearest_centroid.csv\", columns=[\"PassengerId\", \"Survived\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic decision tree Pipeline\n",
    "classifier = Pipeline([\n",
    "        ('classifier', tree.DecisionTreeClassifier())\n",
    "    ])\n",
    "\n",
    "max_depths = [10, 20, 30, 40, 50, 70, 100, 150, 200, 300, 400, 1000]\n",
    "parameters = {\n",
    "    'classifier__max_depth': max_depths, \n",
    "    'classifier__criterion': [\"gini\", \"entropy\"],\n",
    "    'classifier__splitter': [\"best\", \"random\"],\n",
    "    'classifier__min_samples_split':[2, 3, 4, 5, 7, 10, 15, 20, 25],\n",
    "    'classifier__random_state': [1],\n",
    "    'classifier__max_features': [2, 5, 10, 20, 40, \"auto\", \"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "tree_clf = GridSearchCV(classifier, parameters, cv=5)\n",
    "\n",
    "tree_clf.fit(train_x, train_y)\n",
    "\n",
    "print (\"Best parameters found: \")\n",
    "print (tree_clf.best_params_)\n",
    "\n",
    "NFOLDS = 5\n",
    "scores = cross_val_score(tree_clf.best_estimator_, train_x, train_y, cv=NFOLDS)\n",
    "print (\"Expected performance: {:.2f}% (+/-{:.2f}).\".format(np.mean(scores)*100., np.std(scores)*100.))\n",
    "\n",
    "tree_prediction = tree_clf.predict(test_x)\n",
    "\n",
    "# Add the prediction to the test dataset\n",
    "test_classified_with_decision_tree = source_test_df.assign(Survived=list(tree_prediction))\n",
    "\n",
    "# Save to upload to Kaggle\n",
    "test_classified_with_decision_tree.to_csv(\"results/test_classified_with_decision_tree.csv\", columns=[\"PassengerId\", \"Survived\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test values with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Pipeline\n",
    "classifier = Pipeline([\n",
    "        ('minmaxscaler', MinMaxScaler()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5, 7, 10, 15, 20, 25, 30, 40, 50]\n",
    "parameters = {\n",
    "    'classifier__n_neighbors': k_values, \n",
    "    'classifier__weights': [\"uniform\", \"distance\"],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "knn_clf = GridSearchCV(classifier, parameters, cv=5)\n",
    "\n",
    "knn_clf.fit(train_x, train_y)\n",
    "\n",
    "print (\"Best parameters found: \")\n",
    "print (knn_clf.best_params_)\n",
    "\n",
    "NFOLDS = 5\n",
    "scores = cross_val_score(knn_clf.best_estimator_, train_x, train_y, cv=NFOLDS)\n",
    "print (\"Expected performance: {:.2f}% (+/-{:.2f}).\".format(np.mean(scores)*100., np.std(scores)*100.))\n",
    "\n",
    "# Make the prediction over the test set\n",
    "knn_prediction = knn_clf.predict(test_x)\n",
    "\n",
    "# Add the prediction to the test dataset\n",
    "test_classified_with_knn = source_test_df.assign(Survived=list(knn_prediction))\n",
    "\n",
    "# Save to upload to Kaggle\n",
    "test_classified_with_knn.to_csv(\"results/test_classified_with_knn.csv\", columns=[\"PassengerId\", \"Survived\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict test values with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest Pipeline\n",
    "\n",
    "classifier = Pipeline([\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "\n",
    "parameter_grid = {\n",
    "                 'classifier__max_depth' : [4, 6, 12],\n",
    "                 'classifier__n_estimators': [50],\n",
    "                 'classifier__max_features': ['sqrt', 'auto', 'log2'],\n",
    "                 'classifier__min_samples_split': [2, 3, 10],\n",
    "                 'classifier__min_samples_leaf': [1, 3, 10],\n",
    "                 'classifier__bootstrap': [True, False],\n",
    "                 'classifier__n_jobs': [-1]\n",
    "                 }\n",
    "\n",
    "#cross_validation = StratifiedKFold(n_splits=5)\n",
    "\n",
    "grid_search = GridSearchCV(classifier,\n",
    "                           scoring='accuracy',\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=5)\n",
    "\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "random_forest_clf = grid_search\n",
    "parameters = grid_search.best_params_\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "NFOLDS = 5\n",
    "scores = cross_val_score(grid_search.best_estimator_, train_x, train_y, cv=NFOLDS)\n",
    "print (\"Expected performance: {:.2f}% (+/-{:.2f}).\".format(np.mean(scores)*100., np.std(scores)*100.))\n",
    "\n",
    "# Make the prediction over the test set\n",
    "random_forest_prediction = random_forest_clf.predict(test_x)\n",
    "\n",
    "# Add the prediction to the test dataset\n",
    "test_classified_with_rf = source_test_df.assign(Survived=list(random_forest_prediction))\n",
    "\n",
    "# Save to upload to Kaggle\n",
    "test_classified_with_rf.to_csv(\"results/test_classified_with_rf.csv\", columns=[\"PassengerId\", \"Survived\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random forest Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "tuned_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    print \"Best parameters for SVM\"\n",
    "    print clf.best_params_\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    \n",
    "    print('Best parameters: {}'.format(clf.best_params_))\n",
    "\n",
    "    NFOLDS = 5\n",
    "    scores = cross_val_score(clf.best_estimator_, train_x, train_y, cv=NFOLDS)\n",
    "    print (\"Expected performance: {:.2f}% (+/-{:.2f}).\".format(np.mean(scores)*100., np.std(scores)*100.))\n",
    "\n",
    "    # Make the prediction over the test set\n",
    "    svm_prediction = clf.predict(test_x)\n",
    "\n",
    "    # Add the prediction to the test dataset\n",
    "    test_classified_with_svm = source_test_df.assign(Survived=list(svm_prediction))\n",
    "\n",
    "    # Save to upload to Kaggle\n",
    "    test_classified_with_svm.to_csv(\"results/test_classified_with_svm_{}.csv\".format(score), columns=[\"PassengerId\", \"Survived\"], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
